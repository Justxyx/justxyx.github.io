## 1. 实验架构图

![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p1.png)

## 2.实验过程

- 实验环境
  ubuntu18.04 + nvidia 1080(11G)

- 框架环境
  python 3.6.13 + pytorch 1.8.1 + torchvision 0.9.1

- 实验模型介绍

  1. 主干特征提取网络为 Rest18 网络。
  2. Pseudo label 模块采用 Rest50 网络作为预训练网络，生成伪标签。

- 实验过程
  1. 给定一张图片输入 input s(3 _ 100 _ 100)，经过 transform 图像增强为(3 _ 224 _ 224). 分割为 4 张图片 s1(3 _ 56 _ 56),s2(3 _ 56 _ 56),s3(3 _ 56 _ 56),s4(3 _ 56 _ 56),s1(3 _ 56 _ 56) 。 输入到 Res18 网络，分别得到特征为 F F1 F2 F3 F4 以及对应的注意力机制权重值 a1,a2,a3,a4 , a 。 则 F 总 = soft*max(F) * a + soft*max(F1) * a1 +soft*max(F2) * a2 +soft*max(F3) * a3 +soft_max(F4) \* a4 。
  2. 图片输入伪标签生成器，得到 F 伪 。
  3. 计算 F 总 ， F 伪 交叉损失，得到 loss

## 3.数据集介绍

- RAF_DB

  包含 30,000 张面部图像与标注。在我们的实验中，只使用了 6 个基本表达（中性、幸福、惊喜、悲伤、愤怒、厌恶、恐惧）和中性表达的图像。其大小均为 100 \* 100 像素，在我们的实验中，共抽取 12271 张图像用于训练，3068 张图像用于测试。

- FERPlus
  它是由谷歌搜索引擎收集的一个大规模的数据集。它由 28709 张训练图像、3589 张验证图像和 3589 张测试图像组成，其大小均调整为 48×48 像素。包括蔑视，这导致了这个数据集中的 8 个类。
    
- CAER-S
  CAER-S 数据集拥有 65983 张图像的数据集，已分为训练集（44996 个样本)和测试集(20987 个样本）。图像被分为七类，每张图片为 7 类中的一个。

## 4.实验结果

### 4.1 对比实验结果

- RAF_DB
  |Methods||acc|
  |-|-|-|
  |DLP-CNN[1]|| 84.22|
  |IPA2LT[2]|| 86.77|
  |gaCNN[3]|| 85.07|
  |RAN[4]|| 86.90|
  |SCN(baseline)[5]||87.03|
  |**Our**||**86.21**|

- FERPlus
  |Methods||acc|
  |-|-|-|
  |PLD[6]|| 85.1|
  |ResNet+VGG[7]|| 87.4|
  |SeNet[8]|| 88.8|
  |RAN[4]|| 88.55|
  |RAN_VGG16[4]|| 89.16|
  |SCN(baseline)||88.01|
  |**Our**||**86.71**|

- CAER-S
  |Methods||acc|
  |-|-|-|
  |ResNet-18[9]|| xx|
  |ResNet-50[9]|| xx|
  |MobileNet-V2[10]|| xx|
  |Res2Net-50[11]|| xx|
  |CAER-Net-S[12]|| xx|
  |EfficientFace[13]|| xx|
  |**Our**||**xx**|

### 4.2 消融实验

#### RAF_DB

- 整体网络
  acc 86.21
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p14.png)

- 剔除 MUlti-Regions Module
  acc 84.91
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p7.png)

- 剔除 attention Transformation Module
  acc 86.28
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p8.png)
- 剔除 Pseudo Lable Module
  acc 85.89
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p9.png)

备注:**实验表明 bachsize 的增大会一定程度的提高 acc, 因设备制约暂设置为 128**

#### FERPlus

- 整体网络
  acc 86.71
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p5.png)

- 剔除 MUlti-Regions Module
  acc 86.68
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p4.png)

- 剔除 attention Transformation Module
  acc 86.78
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p3.png)

- 剔除 Pseudo Lable Module
  acc 86.4
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p2.png)

### 4.2 参数 a 结果 acc 影响

**F 总 = F1 + a\*F2**

| a   |     | acc   |
| --- | --- | ----- |
| 0.1 | -   | 85.14 |
| 0.2 | -   | 85.56 |
| 0.3 | -   | 85.59 |
| 0.4 | -   | 85.98 |
| 0.5 | -   | 85.72 |
| 0.6 | -   | 86.21 |
| 0.7 | -   | 85.17 |
| 0.8 | -   | 86.08 |
| 0.9 | -   | 85.59 |
| 1.0 | -   | 85.17 |

![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p19.png)

- 0.1
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p6.png)
- 0.2
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p10.png)
- 0.3
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p11.png)
- 0.4
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p12.png)
- 0.5
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p13.png)
- 0.6
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p14.png)
- 0.7
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p15.png)
- 0.8
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p16.png)
- 0.9
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p17.png)
- 1.0
  ![p1](D:\blog\justxyx.github.io\assets\img\2021.8.17\p18.png)

## 引用

[1] Shan Li, Weihong Deng, and JunPing Du. Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. In CVPR, pages 2852–2861, 2017. 1, 5, 8
[2]Jiabei Zeng, Shiguang Shan, Xilin Chen, and Xilin Chen.
Facial expression recognition with inconsistently annotated
datasets. In ECCV, pages 222–237, 2018. 2, 8
[3]Yong Li, Jiabei Zeng, Shiguang Shan, and Xilin Chen. Occlusion aware facial expression recognition using cnn with
attention mechanism. TIP, 28(5):2439–2450, 2018. 8
[4] Kai Wang, Xiaojiang Peng, Jianfei Yang, Debin Meng,
and Yu Qiao. Region attention networks for pose and
occlusion robust facial expression recognition. arXiv
preprint:1905.04075, 2019.
[5]Wang, K.; Peng, X.; Yang, J.; Lu, S.; and Qiao, Y. 2020a.
Suppressing uncertainties for large-scale facial expression
recognition. In CVPR, 6897–6906.
[6] Emad Barsoum, Cha Zhang, Cristian Canton Ferrer, and
Zhengyou Zhang. Training deep networks for facial expression recognition with crowd-sourced label distribution. In
ACM ICMI, pages 279–283, 2016.
[7]Christina Huang. Combining convolutional neural networks
for emotion recognition. In 2017 IEEE MIT Undergraduate
Research Technology Conference (URTC), pages 1–4, 2017.
[8] Samuel Albanie, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Emotion recognition in speech using crossmodal transfer in the wild. arXiv preprint arXiv:1808.05561, 2018.
[9]He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78.
[10]Sandler, M. , et al. "MobileNetV2: Inverted Residuals and Linear Bottlenecks." 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2018).
[11]Gao, S. , et al. "Res2Net: A New Multi-scale Backbone Architecture." IEEE Transactions on Pattern Analysis and Machine Intelligence PP.99(2019):1-1.
[12]Lee, Jiyoung, Seungryong Kim, Sunok Kim, Jungin Park, and Kwanghoon Sohn. 2019. “Context-Aware Emotion Recognition Networks.” In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 10142–51.
[13] Zhao, Zengqun, Qingshan Liu, and Feng Zhou. 2021. “Robust Lightweight Facial Expression Recognition Network with Label Distribution Training.” In AAAI, 3510–19.
